[ { "title": "Takeaway: Unit Testing of Bash Scripts", "url": "/posts/takeaway-unit-testing-of-bash-scripts/", "categories": "Data Engineering Learning Journey", "tags": "bash, unittest", "date": "2023-09-28 13:40:00 +0200", "snippet": "Bash/Shell is a potent scripting language that lets us communicate with our computer’s operating system. In many of my everyday tasks, I rely on Bash to carry out Linux commands and create certain logical processes. While Bash’s capabilities are impressive, it’s important to note that since it deals with low-level operations within the operating system, it can sometimes escalate minor problems into major ones, without any chance to revert the execution.To ensure the reliability of our scripts for daily business operations, it’s crucial to incorporate unit testing into our Bash scripts. This practice helps us maintain stability and prevent unexpected issues from affecting our workflow.Briefly write down my takeaway from Chai Feng’s sharing: https://www.youtube.com/live/-1gB_5dV32U?si=4uICRRG8vDmCKWxE&amp;t=649There are 3 main barriers that prevent us from implementing Bash unit tests. Dependencies, having the exact execution environment can be quite troublesome. The script could have potential side effects and can be tricky to set up or tear down. Command execution is the aim of the script, therefore it’s hard to foresee the result. Execution/validation is not fast enough.Let’s revisit the concept of script’s unit tests. What exactly are they?Is it validating whether the function called runs smoothly? Or to validate the execution logic of the indicated script? It doesn’t matter running commands manually or by a script if the functions resulting errors. So the concept to implement scripts’ unit testing should be to validate the logic, instead of testing if the function can be run.So what should we consider next? The methodology in Bash is essentially as same as other languages unit testing. Don’t bother executing the commands in the PATH because they are external dependencies of the script. Each test instance is independent. Different platforms, different environments, same results. The validation action we expect is to execute the expected command, and send the expected parameters.There are 5 things that determine the execution of commands. In order, they are aliases, keywords, functions, built-in procedures, and external procedures. The most important thing to consider about is how to simulate the command?The testing framework for bash scripts: https://github.com/bach-sh/bach" }, { "title": "Learning and Takeaways from Kubesimplify Workshop", "url": "/posts/learning-and-takeaways-from-kubesimplify-workshop/", "categories": "Data Engineering Learning Journey", "tags": "devops, linux, docker, k8s", "date": "2022-08-07 00:03:00 +0200", "snippet": "This post will keep the learning process and takeaways from the Kubesimplify workshops held by Saiyam Pathak. The motivation for me to catch up on this DevOps topic is to systematically learning by practising for DevOps mindset, in order to achieve double blade stacks, facilitating Data Engineering tasks and projects via DevOps approaches.Workshops List Linux &amp; Docker Fundamentals (Ongoing) Kubernetes 101 GitOps With ArgoCD Kubernetes Security 101 Kubernetes TroubleshootingLinux &amp; Docker FundamentalsInstructor: Chad M. Crowell Lecture&amp;Workshop recording Learning resourceLinux fundamentalI’ve been familiar with most of the commands introduced in this session. I still learnt something new because my learning before wasn’t systematical enough. The takeaways for me in this session are listed below: Knowing the naming and how Linux filesystem works in a clear picture. Pressing control + r in the prompt is able to search historical executions. Under z-shell, there’s a way fancier drop-down list for a more intuitive search. Using Command man to look up the manual of each command. -h isn’t supported for every command, learnt how to use man is a great finding for me. Linux commands are case-sensitive. Simple command without switch on editor echo 'var=\"something\"' &gt; file # To overwrite the fileecho \"var=\"something\"' &gt;&gt; file # To append to the file Create an intermediate directory with -p flag while using mkdir, for example mkdir test/sub-test/error # The prompt would pop out errormkdir -p test/sub-test/correct # Successful execution chmod commands usage https://chmodcommand.com/chmod-600/To be continued…" }, { "title": "Implementing Fivetran Data Source Connector with AWS Lambda", "url": "/posts/implementing-fivetran-data-source-connector-with-aws-lambda/", "categories": "Data Engineering Learning Journey", "tags": "AWS, Lambda, Fivetran, ETL", "date": "2022-05-28 22:47:00 +0200", "snippet": "Basic background of AWS LambdaOfficial developer guide from AWSAWS Lambda is a serverless, event-driven compute service that lets you run code for virtually any type of application or backend service without provisioning or managing servers. In GCP and Azure, we can implement the same idea via Cloud Functions and Azure Funtions.Why is it necessary to understand Lambda to build Fivetran connectors?Fivetran is a reliable data pipeline platform for business users to connect their data source in a convenient way. It provides tons of connectors and integrations for user to choose, like marketing tools, modern databases, etc. Although Fivetran supports amounts of external APIs and data sources integration in default, some of the external APIs we needin reality which doesn’t be supported by Fivetran directly.If you are using AWS, then Lambda stands out at this moment! It allows us to write custom integration functions in Python, Node.js, etc. to approach data integration which not directly supported by Fivetran. In the following use case, connector/ETL was built in Python, connecting an API and Snwoflake. But this article would only focus on the less mentioned parts of the official Fivetran documentation. The basic architecture is shown in the figure below. PrerequisiteFollow the instruction from Fivetran to setup the configuration, click hereLambda’s sample function from Fivetran’s documentimport jsondef lambda_handler(request, context): # Fetch records using api calls (insertTransactions, deleteTransactions, newTransactionCursor) = api_response(request['state'], request['secrets']) # Populate records in insert insert = {} insert['transactions'] = insertTransactions delete = {} delete['transactions'] = deleteTransactions state = {} state['transactionsCursor'] = newTransactionCursor transactionsSchema = {} transactionsSchema['primary_key'] = ['order_id', 'date'] schema = {} schema['transactions'] = transactionsSchema response = {} # Add updated state to response response['state'] = state # Add all the records to be inserted in response response['insert'] = insert # Add all the records to be marked as deleted in response response['delete'] = delete # Add schema defintion in response response['schema'] = schema # Add hasMore flag response['hasMore'] = 'false' return response\tdef api_response(state, secrets): # your api call goes here insertTransactions = [ {\"date\":'2017-12-31T05:12:05Z', \"order_id\":1001, \"amount\":'$1200', \"discount\":'$12'}, {\"date\":'2017-12-31T06:12:04Z', \"order_id\":1001, \"amount\":'$1200', \"discount\":'$12'}, ] deleteTransactions = [ {\"date\":'2017-12-31T05:12:05Z', \"order_id\":1000, \"amount\":'$1200', \"discount\":'$12'}, {\"date\":'2017-12-31T06:12:04Z', \"order_id\":1000, \"amount\":'$1200', \"discount\":'$12'}, ] return (insertTransactions, deleteTransactions, '2018-01-01T00:00:00Z')Multi-pages response implementationFivetran will stop the request if it gets a response has an attribute hasMore which equals 'false'response['hasMore'] = 'false'Which means, more than 1 pages response should be able to switch the value by a pointer. false should be able to altered to make Fivetran know that the request hasn’t finished, the pointer should be updated as well once all pages are done. I’m sharing my implementation below to meet this requirement.import datetimeimport asynciofrom services import processor, cursor_formatterdef handler(event, context): \"\"\" Lambda function handler to handle output format \"\"\" loop = asyncio.get_event_loop() insertLoanApplication, insertApplicants, insertAdvisors, state = \\ loop.run_until_complete(api_response(event['state'], event['secrets'])) if not state['moreData']: loop.close() insert = { 'loan_applications': insertLoanApplication, 'applicants': insertApplicants, 'advisors': insertAdvisors } schema_loan_applications = {'primary_key': ['id']} schema_applicants = {'primary_key': ['loan_application_id', 'id']} schema_advisors = {'primary_key': ['id']} schema = { 'loan_applications': schema_loan_applications, 'applicants': schema_applicants, 'advisors': schema_advisors } return { 'state': state if state['moreData'] else {'cursor': cursor_formatter(datetime.datetime.now()), 'page': 0}, 'insert': insert, 'schema': schema, 'hasMore': 'false' if not state['moreData'] else 'true' }async def api_response(state, secrets): \"\"\" Main function to call indicated API :param state: Fivetran anchor for indexing usage, default None :param secrets: The secret you would like to use to call the API :return: API responses \"\"\" try: cursor_value, page = state['cursor'], state[\"page\"] except KeyError: cursor_value, page = cursor_formatter(datetime.datetime.now()), 0 page += 1 insertLoanApplication, insertApplicant, insertAdvisor, moreData = await processor(secrets, page, cursor_value) state = {'cursor': cursor_value, 'page': page, 'moreData': moreData} return insertLoanApplication, insertApplicant, insertAdvisor, stateFirst, we can see in api_request function, state is assigned by request format, empty dictionary object in default, we can assign any pointer we need for requesting API, see here to check the detail.We retrieve the cursor and page at the beginning, cursor is for locating the timestamp of each response whether we’ve done already, and page is literally for locating which page we are at. Fivetran can tell if this is an initial request, or if it is a request that is still pending and should be continued.try: cursor_value, page = state['cursor'], state[\"page\"]except KeyError: cursor_value, page = cursor_formatter(datetime.datetime.now()), 0After processing, we will get a function return value moreData for the Lambda handler to continue or stop the request.state = {'cursor': cursor_value, 'page': page, 'moreData': moreData}In this example, we have the return value from the handler to present continuing or stopping. Else we have no more new response from API, returning a timestamp and reset the page value to 0 for the next round requesting.return { 'state': state if state['moreData'] else {'cursor': cursor_formatter(datetime.datetime.now()), 'page': 0}, 'insert': insert, 'schema': schema, 'hasMore': 'false' if not state['moreData'] else 'true'}ConclusionSince the usage context is relatively small, Fivetran does not have document that describes how to request a multi-page response from API requesting, so we use several pointers to implement the requirements alone, which still quite fits actually.If you’ve noticed, I’ve also designed asynchronous requests for this purpose to speed up request efficiency, but this also creates a burden on the API server, I’ll share how to optimize requests in a later post." }, { "title": "Useful gadget sharing - cron-job.org", "url": "/posts/useful-gadget-sharing-cron-job-org/", "categories": "Sharing", "tags": "heroku, cron", "date": "2021-04-30 02:00:00 +0200", "snippet": "Useful gadget sharing - cron-job.orgDue to initiating to maintain my side projects which have done before, I started investigating any pain points that needed to be improved or could be divided into small projects.StoryI want to introduce a really cool, simple but useful gadget for you, cron-job.org.The context was that I had a side project which deployed on Heroku. Heroku is a really convenient platform that provides a basic deployment environment for someone who wants to build their website or front-end app interface.In short, Heroku free tier is quite sufficient for me. But your app is put to down after 30 mins of inactivity, and will need around 5–10 seconds to wake up the app again. If your app cannot tolerate that in any circumstance, you may want to try some approaches to avoid your app sleep again but still free. Pinging your app on a set interval might be an economy way to achieve it.Approach Make sure you have a simple GET request to your app homepage. For example, if your app is hosted at your-app.herokuapp.com, make sure you have a GET method handling “/” route. Or, have a GET method at some other route. Once that’s completed, we can either handle pinging our own app manually or automate the process. The Heroku free tier requires applications to sleep a minimum of seven hours every day. Unverified accounts get 550 of free dyno hours per month, which is equal to almost 7 hours of sleep time for a month of 31 days. To be on the safe side, we’ll go with seven hours of sleep. The example GET function writing in Python/Flask@bp.route('/ping', methods=['GET'])def ping(): res = { \"name\": \"CV parser\", \"requested_at\": datetime.datetime.now(), \"status\": \"ok\" } return jsonify(res)Using cronjob.org to keep your app alive Signup on https://cron-job.org/en/ Verify your email once the signup is done. Log in and click on the Cronjobs tab. And click on “Create cronjob”. Fill in the details. Enter URL according to your GET method. For example, my URL will be https://you.herokuapp.com/pingSchedule the execution time from 8 to 23 every 30 minutes every day You can also check the execution detail in dashboard " }, { "title": "Run your own Apache Spark jobs in AWS EMR and S3", "url": "/posts/run-your-own-apache-spark-jobs-in-aws-emr-and-s3/", "categories": "Data Engineering Learning Journey", "tags": "AWS, data, lake, pipeline", "date": "2021-02-01 01:00:00 +0100", "snippet": "Run your own Apache Spark jobs in AWS EMR and S3Recently, I participated in Udacity’s Nanodegree Program for Data Engineers. It’s kinda like to review what I did past and refresh some tech stacks’ knowledge for me in the Data Engineering field. Today, I’m gonna writing this article to avoid you spending extra efforts to run a Spark job or other Hadoop eco-systems jobs on AWS.This idea comes from the Data Lake demo below, you can find the part I tried to apply through Shell script and Makefile between [S3-&gt;EMR-&gt;S3] in the figure. The PySpark script I used reference from the project in Course Data Lake, click the photo to check detail.Before we start, make sure you’ve already done with registering an AWS account and downloading the credential on AWS console(the easiest way, you bet).EMR workflowFirst off, let us have a look at the EMR workflow, click the photo to check detail.What we want to do, is to make sure the work of setting up and launching a cluster, adding working steps, creating log files, and terminating the cluster as simple as we can. AWS CLI(command-line interface) builds an easy interface and functional usage for operating various services on AWS. But we are eager to make entire management much simpler and flexible to check the status, even put it aside until the job is finished.Of course, you have other choices like SDK for different languages, like Python SDK or Java SDK if you like. In my experiences, every SDK sometimes use a different version of libraries or packages, you should be aware of the supporting version on each service you choose.Idea and PracticeWe can choose the easy-checking cli command for pipeline, such as creating-cluster, add-steps, and terminate-clusters.The cli command from document is shown below.aws emr create-cluster \\--name \"My First EMR Cluster\" \\--release-label emr-5.31.0 \\--applications Name=Spark \\--ec2-attributes KeyName=myEMRKeyPairName \\--instance-type m5.xlarge \\--instance-count 3 \\--use-default-rolesSet the command as a variable to load in the next step, add 2 additional arguments --query '[ClusterId]', --output text to change the output format and query the information we need, reference the variable cluster_id in the Shell scripts as below.cluster_id=\"$(aws emr create-cluster --name SparkifyEMR \\--release-label emr-5.31.0 \\--log-uri \"s3://$clustername/logs/\" \\--applications Name=Spark \\--ec2-attributes KeyName=\"$keyname\" \\--instance-type m5.xlarge \\--instance-count 3 \\--use-default-roles \\--query '[ClusterId]' \\--output text)\"echo -e \"\\nClusterId: $cluster_id\"Set the second variable step_id and query it.step_id=\"$(aws emr add-steps \\--cluster-id \"$cluster_id\" \\--steps \"Type=Spark,Name=SparkifyETL,ActionOnFailure=CONTINUE,\\Args=[s3://$clustername/run_etl.py,--data_source,$clustername/data/,--output_uri,$clustername/output/]\" \\--query 'StepIds[0]' \\--output text)\"echo -e \"\\nStepId: $step_id\"Set a while loop to check Spark jobs is finished or not.describe_step=\"$(aws emr describe-step --cluster-id \"$cluster_id\" \\ --step-id \"$step_id\" --query 'Step.Status.State' --output text)\"while true; do if [[ $describe_step = 'PENDING' ]]; then echo \"Cluster creating... Autocheck state 2 mins later\" sleep 120 elif [[ $describe_step = 'RUNNING' ]]; then echo \"Job running... Autocheck state 30 seconds later\" sleep 30 elif [[ $describe_step = 'FAILED' ]]; then echo \"Job failed\" break else echo \"Job completed\" break fidoneAfter finishing all jobs, terminate the cluster of the jobs complete without error.terminate_cluster=\"$(aws emr terminate-clusters \\--cluster-ids \"$cluster_id\" \\--query 'Cluster.Status.State' \\--output text)\"while true; do if [[ $terminate_cluster != 'TERMINATED' ]]; then echo \"Cluster terminating...\" sleep 15 else echo \"Cluster terminated\" fidoneConclusionAnyway, this script brings you a convenient approach to run one-time Spark jobs in AWS EMR Cluster. You can also check the all lines in the script here for reference.if you’d like to try it yourself, follow the tutorial from AWS official documents to build and manage your EMR clusters and jobs." }, { "title": "How to train a customized Name Entities Recognition (NER) model based on spaCy pre-trained model", "url": "/posts/how-to-train-a-customized-name-entities-recognition-ner-model-based-on-spacy-pre-trained-model/", "categories": "Data Engineering Learning Journey", "tags": "NER, spaCy, NLP", "date": "2020-03-04 01:00:00 +0100", "snippet": "How to train a customized Name Entities Recognition (NER) model based on spaCy pre-trained modelThere are a bunch of online resources to teach you how to train your own NER model by spaCy, so I will attach some links for reference and skip this part.spaCy: Training the named entity recognizerConvert SpaCy training data to Dataturks NER JSON outputCustom Named Entity Recognition Using spaCyBack to our main point of this article, and let’s check the following code. I’m trying to train a customized NER model and retain the other pipeline in spaCy, i.e. tagger and parser. You can also add your own pipeline into the model as well.Write a function as below, and load the model if you’ve pointed it out. I used default pre-trained model en_core_web_sm here.def train_spacy(self, training_data, testing_data, dropout, display_freq=1, output_dir=None, new_model_name=\"en_model\"): # create the built-in pipeline components and add them to the pipeline # nlp.create_pipe works for built-ins that are registered with spaCy # create blank Language class if self.model: nlp = spacy.load(self.model) else: nlp = spacy.blank('en')The following is just like the tutorial from spaCy official website, create the pipeline or grab it, then add labels from the training data you’ve annotated already. if 'ner' not in nlp.pipe_names: ner = nlp.create_pipe('ner') nlp.add_pipe(ner, last=True) else: ner = nlp.get_pipe('ner') # add labels for _, annotations in training_data: for ent in annotations.get('entities'): ner.add_label(ent[2])And now, let’s training. The initial loss as 100000, and I set up the early stop to avoid overfitting. # get names of other pipes to disable them during training other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner'] with nlp.disable_pipes(*other_pipes): # only train NER optimizer = nlp.begin_training() print(\"&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Training the model &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\\n\") losses_best = 100000 early_stop = 0Use stochastic gradient descent as optimizer with mini-batch configuration, get more insights here. for itn in range(self.n_iter): print(f\"Starting iteration {itn + 1}\") random.shuffle(training_data) losses = {} batches = minibatch(training_data, size=compounding(4., 32., 1.001)) for batch in batches: text, annotations = zip(*batch) nlp.update( text, # batch of texts annotations, # batch of annotations drop=dropout, # dropout - make it harder to memorise data sgd=optimizer, # callable to update weights losses=losses) if itn % display_freq == 0: print(f\"Iteration {itn + 1} Loss: {losses}\") if losses[\"ner\"] &lt; losses_best: early_stop = 0 losses_best = int(losses[\"ner\"]) else: early_stop += 1 print(f\"Training will stop if the value reached {self.not_improve}, \" f\"and it's {early_stop} now.\\n\") if early_stop &gt;= self.not_improve: break print(\"&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Finished training &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\")After training, dump the model as a binary data to your on-premises disk. Put the last 10 lines inside the with nlp.disable_pipes(*other_pipes): will save the only ner pipeline. Otherwise, the entire model will be saved, that’s exactly what I need. if output_dir: path = output_dir + f\"en_model_ner_{round(losses_best, 2)}\" else: path = os.getcwd() + f\"/lib/inactive_model/en_model_ner_{round(losses_best, 2)}\" os.mkdir(path) if testing_data: self.validate_spacy(model=nlp, data=testing_data) with nlp.use_params(optimizer.averages): nlp.meta[\"name\"] = new_model_name bytes_data = nlp.to_bytes() lang = nlp.meta[\"lang\"] pipeline = nlp.meta[\"pipeline\"] model_data = dict(bytes_data=bytes_data, lang=lang, pipeline=pipeline) with open(path + '/model.pkl', 'wb') as f: pickle.dump(model_data, f)You can load the model by the following function to load your model by your specified path on your disk.def load_model(model_path): with open(model_path + '/model.pkl', 'rb') as f: model = pickle.load(f) nlp = spacy.blank(model['lang']) for pipe_name in model['pipeline']: pipe = nlp.create_pipe(pipe_name) nlp.add_pipe(pipe) nlp.from_bytes(model['bytes_data']) return nlp" }, { "title": "Connect PostgreSQL in docker container with Azure Data Studio", "url": "/posts/connect-postgresql-in-docker-container-with-azure-data-studio/", "categories": "Data Engineering Learning Journey", "tags": "azure-data-studio", "date": "2020-02-05 01:00:00 +0100", "snippet": "Connect PostgreSQL in docker container with Azure Data StudioAzure Data Studio is a cool product that can easily connect MySQL (if you already installed in your system) and show what in your database look like. What if we use PostgresSQL instead of MySQL, how should we start?The 1st step, I decided to pull Postgres’s docker image but not to install at root.$ docker pull postgresSecond, run the container, remember to set the port for local connecting (the document didn’t mention it)$ docker run --name postgres-docker -e POSTGRES_PASSWROD=secret_password -p 5432:5432 -d postgresthen you will get container id, also can check it by docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESba01d334f4db postgres \"docker-entrypoint.s…\" 37 minutes ago Up 37 minutes 0.0.0.0:5432-&gt;5432/tcp postgres-dockerAfter running the container, we could start the next step, set up the connection in Azure Data Studio.1. Install PostgreSQL plug-in.Choose the extension’s icon and search Postgres then install it. 2. Click new connection, choose PostgreSQL as your database. 3. Enter the required information. 4. Remember to set port in advanced setting.Enter the ip and port as same as you assigned to docker container. 5. Connect and start to work in the database. So far, the most important part is that remember to assign port when you run the docker container and enter the correct connection details, then it can keep your life much easier." }, { "title": "Setting both Celery and Flask inside the docker-compose", "url": "/posts/setting-both-celery-and-flask-inside-the-docker-compose/", "categories": "Data Engineering Learning Journey", "tags": "celery, flask, docker", "date": "2020-02-04 01:00:00 +0100", "snippet": "Setting both Celery and Flask inside the docker-composeDue to the issue I need to resolve is that put the heavy task to background, then run it periodically or asynchronously. And the most important thing is that have to support Python.I found Celery, the excellent tool to implement distributed task queue by Python, can easily deal with request using asynchronous designing.It seems like a perfect solution for my issue, however, there has still a new problem arised, what is I need to focus on processing the task ran by Flask and packed in docker and running by docker compose, it is difficult to find all of these resources at the same time on the Internet.So let’s started!My article is referenced from the following resources: Asynchronous Tasks with Flask and Redis Queue The web-service for extracting dominant color from images. Using Celery with Flask Flask + Celery tutorial (Mandarin resource) Task schduling with Celery (Mandarin resource)App Structure DisplayingFlask_Celery_example|├── api| ├── __init__.py| ├── app.py | ├── celery_app.py| ├── config.py| ├── Dockerfile| └── requirements.txt|├── docker-compose.yml└── README.mdSet up your Redis and Celery worker in docker-compose, the breaking point is to set up the celery worker’s name well.version: \"3\"services: web: container_name: web build: ./api ports: - \"5000:5000\" links: - redis depends_on: - redis environment: - FLASK_ENV=development volumes: - ./api:/app redis: container_name: redis image: redis:5.0.5 hostname: redis worker: build: context: ./api hostname: worker entrypoint: celery command: -A celery_app.celery worker --loglevel=info volumes: - ./api:/app links: - redis depends_on: - redisWhat the Dockerfile in this example looks like.FROM python:3.6.5WORKDIR /appCOPY requirements.txt ./RUN pip install --no-cache-dir -r requirements.txtCOPY . .EXPOSE 5000CMD [ \"python\", \"./app.py\" ]Initialize Celery worker and create asynchronous or scheduled task in celery_app.pyimport configfrom celery import Celery# Initialize Celerycelery = Celery( 'worker', broker=config.CeleryConfig.CELERY_BROKER_URL, backend=config.CeleryConfig.CELERY_RESULT_BACKEND)@celery.task()def func1(arg): ... return ...Run task by Flask and check its status in app.pyimport configfrom celery_app import func1from flask import Flask, request, jsonify, url_for, redirect# Your API definitionapp = Flask(__name__)app.config.from_object('config.FlaskConfig')@app.route('/', methods=['POST'])def longtask(): task = func1.delay(arg) return redirect(url_for('taskstatus', task_id=task.id)) @app.route('/status/&lt;task_id&gt;')def taskstatus(task_id): task = func1.AsyncResult(task_id) if task.state == 'PENDING': time.sleep(config.SERVER_SLEEP) response = { 'queue_state': task.state, 'status': 'Process is ongoing...', 'status_update': url_for('taskstatus', task_id=task.id) } else: response = { 'queue_state': task.state, 'result': task.wait() } return jsonify(response)if __name__ == '__main__': app.run()After finishing the all setting, we can run the script on terminal below to create container and run it.$ cd Flask_Celery_example$ docker-compose build$ docker-compose runConclusionThe bottleneck in this case is that I could run the Flask, Redis, Celery separately in docker-compose at first, but if I want to run it with only one script then it failed. I had tried and error lots of times to finding the breaking point, the correct script to run Celery in docker-compose. Everything was clear after throughing this bottleneck.Thanks for reading my first article! Leave your message below my Facebook post if there’s still any further questions. See you then." } ]
